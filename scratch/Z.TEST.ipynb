{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passive-medicine",
   "metadata": {},
   "source": [
    "FastText Pre-trained 한국어 모델 사용기\n",
    "- https://inahjeon.github.io/fasttext/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reflected-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "path = 'resource/cc.ko.300.bin'\n",
    "ko_model = models.fasttext.load_facebook_model(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "soviet-senate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 0.565061628818512\n",
      "자이썬: 0.5624369382858276\n",
      "레일스: 0.5598082542419434\n",
      "파이썬을: 0.5595801472663879\n",
      "언어용: 0.5288202166557312\n",
      "파이썬의: 0.5250024795532227\n",
      "프로그래밍: 0.5225088000297546\n",
      "wxPython: 0.5222088098526001\n",
      "파이썬이나: 0.5201171636581421\n",
      "함수형: 0.5187377333641052\n"
     ]
    }
   ],
   "source": [
    "for w, sim in ko_model.similar_by_word('파이썬', 10):\n",
    "    print(f'{w}: {sim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sustainable-contrast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = ko_model.wv.get_vector('과일')\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "burning-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# FastText Word2Vec 관련 라이브러리\n",
    "##################################\n",
    "from scipy import spatial\n",
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "pretrained_kr_word2vec = 'resource/ko.bin'\n",
    "# datapath = 'ko.bin'\n",
    "datapath = pretrained_kr_word2vec\n",
    "# print(datapath)\n",
    " \n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "word2vec = gensim.models.Word2Vec.load(datapath)\n",
    "\n",
    "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,형용사, 부사, 알파벳 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Adjective\", \"Adverb\"], stopword=[]):\n",
    "    return [\n",
    "        word for word, tag in Okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]\n",
    "\n",
    "\n",
    "from gensim import models\n",
    "def get_fasttext_model(path):\n",
    "    if type(ko_model) == gensim.models.fasttext.FastText:\n",
    "        print('Model is already loaded')\n",
    "    else:\n",
    "        path = 'resource/cc.ko.300.bin'\n",
    "    ko_model = get_fasttext_model(path)            \n",
    "    ko_model = models.fasttext.load_facebook_model(path)\n",
    "    return ko_model\n",
    "\n",
    "\n",
    "def sim_sentences_fasttext_word2vec(ko_model, sentences):\n",
    "    '''\n",
    "    word2vect의 리터하는 벡터들의 평균을 구하고, 이를 코사인 거리를 구함\n",
    "    '''\n",
    "    def get_word2vec(ko_model, words, embedding_size=300):\n",
    "        '''\n",
    "        단어 리스트를 받고, 벡터로 변환후에 모든 벡터의 평균을 리턴\n",
    "        '''\n",
    "        vectors = []\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vector = ko_model.wv.get_vector(word)\n",
    "                # print(word)\n",
    "            except:\n",
    "                # print(\"Unknowd words\")\n",
    "                vector = np.zeros(embedding_size) # 모르는 단어이면 0으로 채움\n",
    "            # break\n",
    "            vectors.append(vector)\n",
    "            # print(vector.sum())     \n",
    "\n",
    "        avg_vectors = np.mean(vectors, axis=0)    \n",
    "        return avg_vectors\n",
    "    \n",
    "    s1 = sentences[0]\n",
    "    s2 = sentences[1]\n",
    "    ps1 = tokenizer(s1)\n",
    "    ps2 = tokenizer(s2)\n",
    "\n",
    "    avg_vector1 = get_word2vec(ko_model, ps1)\n",
    "    avg_vector2 = get_word2vec(ko_model, ps2)\n",
    "    # print(avg_vector)\n",
    "    dist_w2v = spatial.distance.cosine(avg_vector1, avg_vector2)\n",
    "    print(s1,',', s2)\n",
    "    print('dist_w2v: {}'.format(dist_w2v))    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-triple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "thousand-commander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과는 과일이다 , 책은 인류가 쌓은 지식의 보고다\n",
      "dist_w2v: 0.680292159318924\n",
      "사과는 과일이다 , 건강에 좋은 것은 과일이다\n",
      "dist_w2v: 0.3414410352706909\n"
     ]
    }
   ],
   "source": [
    "s1 ='사과는 과일이다'\n",
    "s2 = '책은 인류가 쌓은 지식의 보고다'\n",
    "s3 = '건강에 좋은 것은 과일이다'\n",
    "\n",
    "sents1 = [s1, s2]    \n",
    "sents2 = [s1, s3]    \n",
    "\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents1)\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "subtle-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is already loaded\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "def get_fasttext_model(ko_model, path):\n",
    "    '''\n",
    "    path = 'resource/cc.ko.300.bin'\n",
    "    ko_model = get_fasttext_model(path)    \n",
    "    '''    \n",
    "    try:\n",
    "        if type(ko_model) == gensim.models.fasttext.FastText:\n",
    "            print('Model is already loaded')\n",
    "    except:\n",
    "        print('Model is loading')\n",
    "        ko_model = models.fasttext.load_facebook_model(path)\n",
    "        \n",
    "    return ko_model        \n",
    "\n",
    "path = 'resource/cc.ko.300.bin'\n",
    "ko_model = get_fasttext_model(ko_model, path)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "anticipated-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과는 과일이다 , 책은 인류가 쌓은 지식의 보고다\n",
      "dist_w2v: 0.680292159318924\n",
      "사과는 과일이다 , 건강에 좋은 것은 과일이다\n",
      "dist_w2v: 0.3414410352706909\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ko_model == None\n",
    "except:\n",
    "    ko_model = None\n",
    "    path = 'resource/cc.ko.300.bin'    \n",
    "    ko_model = get_fasttext_model(ko_model, path)    \n",
    "\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents1)\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-night",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
