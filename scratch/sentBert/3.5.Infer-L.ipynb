{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bored-improvement",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seventh-adapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "special-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', 'gluonnlp', 'torch', 'sentencepiece', 'tqdm', \n",
    "                 'onnxruntime', 'transformers', 'git+https://git@github.com/SKTBrain/KoBERT.git@master'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-vault",
   "metadata": {},
   "source": [
    "## KoBERT 라이브러리 등 로딩\n",
    "\n",
    "만약에 `import model` 에서 에러가 발생할 경우에 Kernel 리스타트 하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laden-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import nd, gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from model import get_mxnet_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "from bert import BERTDatasetTransform, BERTDataset, BERTClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-universal",
   "metadata": {},
   "source": [
    "## 사용할 GPU 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "monetary-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUS: 4\n",
      "GPU is assigned\n"
     ]
    }
   ],
   "source": [
    "num_gpus = mx.context.num_gpus()\n",
    "ctx = mx.cpu(0)\n",
    "print(\"Number of GPUS:\" , num_gpus)\n",
    "if num_gpus > 0:\n",
    "    ctx = mx.gpu(0)\n",
    "    print(\"GPU is assigned\")\n",
    "else:\n",
    "    ctx = mx.cpu(0)\n",
    "    print(\"CPU is assigned\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-classics",
   "metadata": {},
   "source": [
    "## KoBERT 모델 및 vocab 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radical-hamburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocab = get_mxnet_kobert_model(use_decoder=False, \n",
    "                                          use_classifier=False, \n",
    "                                          ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-topic",
   "metadata": {},
   "source": [
    "## Sentence Classification Classifier 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "federal-editing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of classes:  2\n"
     ]
    }
   ],
   "source": [
    "all_labels = ['0','1'] \n",
    "param_path = 'model_save/net_nsmc.params'\n",
    "\n",
    "def get_bert_classifier(bert_base, classes, ctx, model_params_path):\n",
    "    num_classes = len(classes)\n",
    "    print(\"num of classes: \", num_classes)\n",
    "    bert_classifier = nlp.model.BERTClassifier(bert_base, \n",
    "                                               num_classes=num_classes, \n",
    "                                               dropout=0.5)\n",
    "\n",
    "    # Only need to initialize the classifier layer.\n",
    "    bert_classifier.classifier.initialize(init= mx.init.Normal(0.02), ctx= ctx)\n",
    "    bert_classifier.hybridize(static_alloc=True)\n",
    "    bert_classifier.load_parameters(model_params_path)    \n",
    "\n",
    "    return bert_classifier\n",
    "\n",
    "ko_sent_sims_classifier = get_bert_classifier(bert_base, \n",
    "                                          all_labels, \n",
    "                                          ctx, \n",
    "                                          param_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-smile",
   "metadata": {},
   "source": [
    "## 토큰나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "grateful-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer() # kobert 토큰나이저\n",
    "kobert_tokenizer = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-conviction",
   "metadata": {},
   "source": [
    "## Data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stuffed-friendly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['하루한끼 깻잎', '밀양 깻잎', '1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = nlp.data.TSVDataset('preproc/L-test/test.tab', \n",
    "                                     field_indices=[1,2,3], \n",
    "                                     num_discard_samples=1)\n",
    "dataset_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "further-covering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 4937, 7828, 5649,  517,    0, 7146,    3, 2181, 6853,  517,\n",
       "           0, 7146,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1], dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array(14, dtype=int32),\n",
       " array([1], dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert2 import data\n",
    "vocabulary = vocab\n",
    "def transform_bert_input_type(dataset, idx, bert_tokenizer, max_seq_length=100, pair=True):\n",
    "    '''\n",
    "    자연어 입력값인 sentence1, sentence2, label 을\n",
    "    아래와 같은 버트 입력 형태로 변경\n",
    "    token_id\n",
    "    segment_id\n",
    "    valid_length\n",
    "    label\n",
    "    '''\n",
    "    # The labels for the two classes [(0 = not similar) or  (1 = similar)]\n",
    "    all_labels = [\"0\", \"1\"]\n",
    "\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    # for regression task, set class_labels=None\n",
    "    # for inference without label available, set has_label=False\n",
    "    pair = True\n",
    "    transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_seq_length,\n",
    "                                                    class_labels=all_labels,\n",
    "                                                    has_label=True,\n",
    "                                                    pad=True,\n",
    "                                                    pair=pair)    \n",
    "\n",
    "    data_train = dataset.transform(transform)\n",
    "\n",
    "    return data_train\n",
    "    \n",
    "\n",
    "data_test = transform_bert_input_type(dataset_test, 0, kobert_tokenizer)\n",
    "data_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-branch",
   "metadata": {},
   "source": [
    "## 샘플 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "resistant-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : @@@ INCORRECT Prediction @@@ \n",
      "######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\n",
      "['산지뚝심) 금산 추부 GAP 깻잎', '밀양 깻잎', '1']\n",
      "### Token ID ###\n",
      "[   2 2640 7318 5985 6745  517   40 1235 6516 4541 6398  650  266  517\n",
      "    0 7146    3 2181 6853  517    0 7146    3    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ### \n",
      "[[ 1.8476654 -2.5164464]]\n",
      "<NDArray 1x2 @gpu(0)>\n",
      "\n",
      "\n",
      "### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ### \n",
      "[0.98743397 0.01256604]\n",
      "<NDArray 2 @gpu(0)>\n",
      "\n",
      "\n",
      "Actual Value, Pred Value: 1 , 0\n",
      "\n",
      "\n",
      "\n",
      "3 : @@@ INCORRECT Prediction @@@ \n",
      "######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\n",
      "['산지뚝심) 금산 추부 GAP 깻잎', '밀양 깻잎', '1']\n",
      "### Token ID ###\n",
      "[   2 2640 7318 5985 6745  517   40 1235 6516 4541 6398  650  266  517\n",
      "    0 7146    3 2181 6853  517    0 7146    3    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ### \n",
      "[[ 1.8476654 -2.5164464]]\n",
      "<NDArray 1x2 @gpu(0)>\n",
      "\n",
      "\n",
      "### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ### \n",
      "[0.98743397 0.01256604]\n",
      "<NDArray 2 @gpu(0)>\n",
      "\n",
      "\n",
      "Actual Value, Pred Value: 1 , 0\n",
      "\n",
      "\n",
      "\n",
      "5 : @@@ INCORRECT Prediction @@@ \n",
      "######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\n",
      "['산지뚝심) 금산 추부 GAP 깻잎', '밀양 깻잎', '1']\n",
      "### Token ID ###\n",
      "[   2 2640 7318 5985 6745  517   40 1235 6516 4541 6398  650  266  517\n",
      "    0 7146    3 2181 6853  517    0 7146    3    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ### \n",
      "[[ 1.8476654 -2.5164464]]\n",
      "<NDArray 1x2 @gpu(0)>\n",
      "\n",
      "\n",
      "### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ### \n",
      "[0.98743397 0.01256604]\n",
      "<NDArray 2 @gpu(0)>\n",
      "\n",
      "\n",
      "Actual Value, Pred Value: 1 , 0\n",
      "\n",
      "\n",
      "\n",
      "7 : @@@ INCORRECT Prediction @@@ \n",
      "######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\n",
      "['산지뚝심) 금산 추부 GAP 깻잎', '밀양 깻잎', '1']\n",
      "### Token ID ###\n",
      "[   2 2640 7318 5985 6745  517   40 1235 6516 4541 6398  650  266  517\n",
      "    0 7146    3 2181 6853  517    0 7146    3    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ### \n",
      "[[ 1.8476654 -2.5164464]]\n",
      "<NDArray 1x2 @gpu(0)>\n",
      "\n",
      "\n",
      "### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ### \n",
      "[0.98743397 0.01256604]\n",
      "<NDArray 2 @gpu(0)>\n",
      "\n",
      "\n",
      "Actual Value, Pred Value: 1 , 0\n",
      "\n",
      "\n",
      "\n",
      "8 : @@@ INCORRECT Prediction @@@ \n",
      "######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\n",
      "[\"Hav'eat 경상도 친환경 깻잎\", '산지뚝심) 금산 추부 GAP 깻잎', '1']\n",
      "### Token ID ###\n",
      "[   2  652  367  453  520  517  389  377  953 6527 5859 4629  517    0\n",
      " 7146    3 2640 7318 5985 6745  517   40 1235 6516 4541 6398  650  266\n",
      "  517    0 7146    3    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ### \n",
      "[[ 1.0997158 -1.5668627]]\n",
      "<NDArray 1x2 @gpu(0)>\n",
      "\n",
      "\n",
      "### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ### \n",
      "[0.93502545 0.06497452]\n",
      "<NDArray 2 @gpu(0)>\n",
      "\n",
      "\n",
      "Actual Value, Pred Value: 1 , 0\n",
      "\n",
      "\n",
      "\n",
      "9 : @@@ INCORRECT Prediction @@@ \n",
      "######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\n",
      "['산지뚝심) 금산 추부 GAP 깻잎', '밀양 깻잎', '1']\n",
      "### Token ID ###\n",
      "[   2 2640 7318 5985 6745  517   40 1235 6516 4541 6398  650  266  517\n",
      "    0 7146    3 2181 6853  517    0 7146    3    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n",
      "    1    1]\n",
      "### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ### \n",
      "[[ 1.8476654 -2.5164464]]\n",
      "<NDArray 1x2 @gpu(0)>\n",
      "\n",
      "\n",
      "### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ### \n",
      "[0.98743397 0.01256604]\n",
      "<NDArray 2 @gpu(0)>\n",
      "\n",
      "\n",
      "Actual Value, Pred Value: 1 , 0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_sample(sent_classifier, dataset_test, data_test, num_test=3, verbose=False):\n",
    "    # num_test = len(data_test)\n",
    "    # num_test = 3\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    def print_detail(raw_input, bert_input, logit, prob, actual_label, pred_label):\n",
    "        print(\"######### 문장1 , 문장2, 유사도 [0: No, 1: Yes] ########\")\n",
    "        print(raw_input)    \n",
    "        print(\"### Token ID ###\")        \n",
    "        print(bert_input)\n",
    "        print(\"### Logit: 마지막 2개의 뉴런의 값  [0:유사 안함, 1: 유사] ###\", logit)   \n",
    "        print('\\n')\n",
    "        print(\"### Probability: 마지막 2개의 확률 값 [0:유사 안함, 1: 유사]  ###\", prob)   \n",
    "        print('\\n')\n",
    "        print(f\"Actual Value, Pred Value: {actual_label} , {pred_label}\")\n",
    "        print('\\n\\n')        \n",
    "    \n",
    "    for i in range(num_test):   \n",
    "        input_ids = mx.nd.array([data_test[i][0]])\n",
    "        token_type_ids = mx.nd.array([data_test[i][1]])\n",
    "        valid_length = mx.nd.array([data_test[i][2]])\n",
    "\n",
    "        input_ids = input_ids.as_in_context(ctx)\n",
    "        token_type_ids = token_type_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "\n",
    "        logit = sent_classifier.forward(input_ids, token_type_ids, valid_length)\n",
    "        prob = mx.nd.softmax(logit[0])\n",
    "        k = [1]\n",
    "        pred_label = prob.argsort()[-k[-1]:][::-1].astype(int).asnumpy()[0] # Predict Label\n",
    "\n",
    "        actual_label = int(data_test[i][-1]) # Actual Value\n",
    "        if actual_label != pred_label:\n",
    "            print(f\"{i} : @@@ INCORRECT Prediction @@@ \")\n",
    "            print_detail(dataset_test[i], data_test[i][0], logit, prob, actual_label, pred_label)\n",
    "            \n",
    "        elif verbose:\n",
    "            print(f\"{i} : %%% CORRECT Prediction %%%%\")            \n",
    "            print_detail(dataset_test[i], data_test[i][0], logit, prob, actual_label, pred_label)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "# eval_sample(ko_sent_sims_classifier, dataset_test, data_test, num_test=20, verbose=True)        \n",
    "eval_sample(ko_sent_sims_classifier, dataset_test, data_test, num_test=20, verbose=False)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-possession",
   "metadata": {},
   "source": [
    "## 토큰 분석\n",
    "문장이 어떻게 토큰화 되었는지를 확인 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "focused-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def analyze_tokens(vocab, dataset_test, data_test, sample_id):\n",
    "    idx2token = vocab.idx_to_token\n",
    "    print(dataset_test[sample_id])\n",
    "    \n",
    "    tokens = []\n",
    "    for i, idx in enumerate(data_test[sample_id][0]):\n",
    "        tokens.append(idx2token[idx])\n",
    "        # print(idx2token[idx])    \n",
    "        if i == 32:\n",
    "            break\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "closed-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['산지뚝심) 금산 추부 GAP 깻잎', '밀양 깻잎', '1']\n",
      "['[CLS]', '▁산', '지', '뚝', '심', '▁', ')', '▁금', '산', '▁추', '부', '▁G', 'AP', '▁', '[UNK]', '잎', '[SEP]', '▁밀', '양', '▁', '[UNK]', '잎', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "[\"Hav'eat 경상도 친환경 깻잎\", '산지뚝심) 금산 추부 GAP 깻잎', '1']\n",
      "['[CLS]', '▁H', 'a', 'v', \"▁'\", '▁', 'e', 'at', '▁경', '상', '도', '▁친환경', '▁', '[UNK]', '잎', '[SEP]', '▁산', '지', '뚝', '심', '▁', ')', '▁금', '산', '▁추', '부', '▁G', 'AP', '▁', '[UNK]', '잎', '[SEP]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "analyze_tokens(vocab, dataset_test, data_test, sample_id=1)\n",
    "analyze_tokens(vocab, dataset_test, data_test, sample_id=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-advance",
   "metadata": {},
   "source": [
    "## 종합 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "double-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "0:Non-Similar       0.62      1.00      0.77        10\n",
      "    1:Similar       1.00      0.40      0.57        10\n",
      "\n",
      "     accuracy                           0.70        20\n",
      "    macro avg       0.81      0.70      0.67        20\n",
      " weighted avg       0.81      0.70      0.67        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataset_test, data_test, labels, ctx):\n",
    "    \n",
    "    num_test = len(data_test)\n",
    "    # num_test = 10\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for i in range(num_test):   \n",
    "        # print(dataset_test[i])\n",
    "        \n",
    "        input_ids = mx.nd.array([data_test[i][0]])\n",
    "        token_type_ids = mx.nd.array([data_test[i][1]])\n",
    "        valid_length = mx.nd.array([data_test[i][2]])\n",
    "\n",
    "        input_ids = input_ids.as_in_context(ctx)\n",
    "        token_type_ids = token_type_ids.as_in_context(ctx)\n",
    "        valid_length = valid_length.as_in_context(ctx)\n",
    "\n",
    "        tid_true = int(data_test[i][-1])\n",
    "        logit = ko_sent_sims_classifier.forward(input_ids, token_type_ids, valid_length)\n",
    "        # print(\"last logit: \", logit)\n",
    "        prob = mx.nd.softmax(logit[0])\n",
    "        # print(\"prob: \", prob)\n",
    "        k = [1]\n",
    "        topk_pred = prob.argsort()[-k[-1]:][::-1].astype(int).asnumpy()\n",
    "        y_true.append(tid_true)\n",
    "        y_pred.append(topk_pred[0])\n",
    "        \n",
    "    from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score  \n",
    "    prec = precision_score(y_true, y_pred, labels=labels, average=None)\n",
    "    f1 = f1_score(y_true, y_pred, labels=labels, average=None)\n",
    "    rec = recall_score(y_true, y_pred, labels=labels, average=None)    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(classification_report(y_true, y_pred, target_names = ['0:Non-Similar','1:Similar']))\n",
    "\n",
    "\n",
    "labels=[0,1]\n",
    "evaluate_model(ko_sent_sims_classifier, dataset_test, data_test, labels, ctx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-boutique",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
