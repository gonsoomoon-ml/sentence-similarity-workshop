{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "humanitarian-bargain",
   "metadata": {},
   "source": [
    "# 버트 임베팅으로 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unknown-dryer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.call([sys.executable, '-m', 'pip', 'install', 'gluonnlp', 'torch', 'sentencepiece', 'tqdm', \n",
    "                 'onnxruntime', 'transformers', 'git+https://git@github.com/SKTBrain/KoBERT.git@master'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-inventory",
   "metadata": {},
   "source": [
    "## KoBERT 라이브러리 등 로딩\n",
    "\n",
    "만약에 `import model` 에서 에러가 발생할 경우에 Kernel 리스타트 하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sacred-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import nd, gluon, autograd\n",
    "import gluonnlp as nlp\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "from model import get_mxnet_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "from bert import BERTDatasetTransform, BERTDataset, BERTClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-inventory",
   "metadata": {},
   "source": [
    "## 사용할 GPU 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "curious-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUS: 4\n",
      "GPU is assigned\n"
     ]
    }
   ],
   "source": [
    "num_gpus = mx.context.num_gpus()\n",
    "ctx = mx.cpu(0)\n",
    "print(\"Number of GPUS:\" , num_gpus)\n",
    "if num_gpus > 0:\n",
    "    ctx = mx.gpu(0)\n",
    "    print(\"GPU is assigned\")\n",
    "else:\n",
    "    ctx = mx.cpu(0)\n",
    "    print(\"CPU is assigned\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-tournament",
   "metadata": {},
   "source": [
    "## KoBERT 모델 및 vocab 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latest-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "kr_bert_base, vocab = get_mxnet_kobert_model(use_decoder=False, \n",
    "                                          use_classifier=False, \n",
    "                                          ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-transcription",
   "metadata": {},
   "source": [
    "## 버트 토큰나이저 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bizarre-asbestos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer() # kobert 토큰나이저\n",
    "kobert_tokenizer = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unlike-domain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁오', '뚜', '기', '▁잔', '라면', '▁5', '개', '▁패키지']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kobert_tokenizer(\"오뚜기 잔라면 5개 패키지\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-scout",
   "metadata": {},
   "source": [
    "## 버트 입력 트랜스포머 생성 \n",
    "- 자연어 --> 버트 입력 형태로 변경 하는 변환기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "virgin-inspiration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 3417, 5984, 5561, 3941, 6009,  611, 5357, 4820,    3],\n",
       "       dtype=int32),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array(10, dtype=int32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert2 import data\n",
    "\n",
    "pair = True\n",
    "max_seq_length = 64\n",
    "transform = data.transform.BERTDatasetTransform(kobert_tokenizer, \n",
    "                                                max_seq_length,\n",
    "                                                has_label=False,\n",
    "                                                pad=False,\n",
    "                                                pair=False)    \n",
    "transform([\"오뚜기 잔라면 5개 패키지\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-jamaica",
   "metadata": {},
   "source": [
    "## 샘플 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "confused-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(sent, kobert_model, bert_input_transform, bert_tokenizer, verbose=False):\n",
    "    '''\n",
    "    버트의 문장, 토큰 임베팅 추출\n",
    "    '''\n",
    "\n",
    "    sample = bert_input_transform(sent)\n",
    "\n",
    "    input_ids = mx.nd.array([sample[0]])\n",
    "    token_type_ids = mx.nd.array([sample[1]])\n",
    "    valid_length = mx.nd.array([sample[2]])\n",
    "\n",
    "    input_ids = input_ids.as_in_context(ctx)\n",
    "    token_type_ids = token_type_ids.as_in_context(ctx)\n",
    "    valid_length = valid_length.as_in_context(ctx)\n",
    "\n",
    "    tokens_em, cls_em,  = kobert_model.forward(input_ids, token_type_ids, valid_length)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Orginal Sentence: \", sent)\n",
    "        print(\"Tokens: \", bert_tokenizer(sent[0]))        \n",
    "        print(\"Bert Input Transformation: \", sample)    \n",
    "    \n",
    "    return tokens_em.asnumpy(), cls_em.asnumpy()\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "def eval_cosine_dist(vec1, vec2, sent1, sent2, verbose=False ):\n",
    "    '''\n",
    "    벡터 차원이 3이면 모든 벡터를 평균을 냄. 이 조건은 토큰 벡터임\n",
    "    벡터 차원이 2이면 단순히 벡터 거리 비교\n",
    "    '''\n",
    "    vector_type = None\n",
    "    if (np.ndim(vec1) > 2) &  (np.ndim(vec2) > 2):\n",
    "        avg_vec1 = np.mean(vec1, axis=1)\n",
    "        avg_vec2 = np.mean(vec2, axis=1)\n",
    "        dist = spatial.distance.cosine(avg_vec1, avg_vec2)\n",
    "        vector_type = \"token vectors\"\n",
    "        \n",
    "    else:\n",
    "        dist = spatial.distance.cosine(vec1, vec2)\n",
    "        vector_type = \"sentence vector\"        \n",
    "\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"vector type: \", vector_type)\n",
    "        print(f\"sent1, sent2: {sent1}, {sent2}\")\n",
    "        print('class dist: {0}'.format(dist))            \n",
    "        print('\\n')                    \n",
    "        \n",
    "        \n",
    "    return dist\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raised-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal Sentence:  ['오뚜기 매운 진라면 5개 입']\n",
      "Tokens:  ['▁오', '뚜', '기', '▁매', '운', '▁진', '라면', '▁5', '개', '▁입']\n",
      "Bert Input Transformation:  (array([   2, 3417, 5984, 5561, 1986, 7010, 4360, 6009,  611, 5357, 3836,\n",
      "          3], dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32), array(12, dtype=int32))\n",
      "vector type:  sentence vector\n",
      "sent1, sent2: ['오뚜기 매운 진라면 5개 입'], ['오뚜기 진라면 5개 패키지']\n",
      "class dist: 0.3838374614715576\n",
      "\n",
      "\n",
      "vector type:  token vectors\n",
      "sent1, sent2: ['오뚜기 매운 진라면 5개 입'], ['오뚜기 진라면 5개 패키지']\n",
      "class dist: 0.08472758531570435\n",
      "\n",
      "\n",
      "vector type:  sentence vector\n",
      "sent1, sent2: ['오뚜기 매운 진라면 5개 입'], ['농심 신라면 5개 입']\n",
      "class dist: 0.20076775550842285\n",
      "\n",
      "\n",
      "vector type:  token vectors\n",
      "sent1, sent2: ['오뚜기 매운 진라면 5개 입'], ['농심 신라면 5개 입']\n",
      "class dist: 0.12713992595672607\n",
      "\n",
      "\n",
      "vector type:  sentence vector\n",
      "sent1, sent2: ['오뚜기 매운 진라면 5개 입'], ['오뚜기 매운 진라면 10개 입']\n",
      "class dist: 0.06314051151275635\n",
      "\n",
      "\n",
      "vector type:  token vectors\n",
      "sent1, sent2: ['오뚜기 매운 진라면 5개 입'], ['오뚜기 매운 진라면 10개 입']\n",
      "class dist: 0.011301159858703613\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011301159858703613"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent1 = ['오뚜기 매운 진라면 5개 입']\n",
    "sent2 = ['오뚜기 진라면 5개 패키지']\n",
    "sent3 = ['농심 신라면 5개 입']\n",
    "sent4 = ['오뚜기 매운 진라면 10개 입']\n",
    "\n",
    "tokens_em1, cls_em1 = eval_sample(sent1, kr_bert_base, transform, kobert_tokenizer, verbose=True)            \n",
    "tokens_em2, cls_em2 = eval_sample(sent2, kr_bert_base, transform, kobert_tokenizer)            \n",
    "tokens_em3, cls_em3 = eval_sample(sent3, kr_bert_base, transform, kobert_tokenizer)            \n",
    "tokens_em4, cls_em4 = eval_sample(sent4, kr_bert_base, transform, kobert_tokenizer)            \n",
    "\n",
    "eval_cosine_dist(cls_em1, cls_em2, sent1, sent2, verbose=True )\n",
    "eval_cosine_dist(tokens_em1, tokens_em2, sent1, sent2, verbose=True )\n",
    "\n",
    "eval_cosine_dist(cls_em1, cls_em3, sent1, sent3, verbose=True )\n",
    "eval_cosine_dist(tokens_em1, tokens_em3, sent1, sent3, verbose=True )\n",
    "\n",
    "eval_cosine_dist(cls_em1, cls_em4, sent1, sent4, verbose=True )\n",
    "eval_cosine_dist(tokens_em1, tokens_em4, sent1, sent4, verbose=True )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
