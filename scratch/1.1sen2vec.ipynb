{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supreme-precipitation",
   "metadata": {},
   "source": [
    "# send2vec, word2vect 테스트\n",
    "- How to Compute Sentence Similarity Using BERT and Word2Vec\n",
    "    - https://towardsdatascience.com/how-to-compute-sentence-similarity-using-bert-and-word2vec-ab0663a5d64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "suspended-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# sent2vec 라이브러리\n",
    "##################################\n",
    "\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from scipy import spatial\n",
    "\n",
    "def sim_sentences_sent2vec(sentences):\n",
    "    vectorizer = Vectorizer()\n",
    "    print(sentences)\n",
    "    vectorizer.bert(sentences)\n",
    "    vectors = vectorizer.vectors    \n",
    "    dist_1 = spatial.distance.cosine(vectors[0], vectors[1])\n",
    "    # dist_2 = spatial.distance.cosine(vectors[0], vectors[2])\n",
    "    # print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))\n",
    "    print('dist_1: {0}'.format(dist_1))    \n",
    "    # dist_1: 0.043, dist_2: 0.192\n",
    "\n",
    "    \n",
    "##################################\n",
    "# KoNLpy 토큰라이저\n",
    "##################################\n",
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "pretrained_kr_word2vec = 'resource/ko.bin'\n",
    "# datapath = 'ko.bin'\n",
    "datapath = pretrained_kr_word2vec\n",
    "# print(datapath)\n",
    " \n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "word2vec = gensim.models.Word2Vec.load(datapath)\n",
    "\n",
    "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,형용사, 부사, 알파벳 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Adjective\", \"Adverb\"], stopword=[]):\n",
    "    return [\n",
    "        word for word, tag in Okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]\n",
    "\n",
    "    \n",
    "##################################\n",
    "# Kakao 개발자 Word2Vec 관련 라이브러리\n",
    "##################################\n",
    "\n",
    "    \n",
    "def sim_sentences_word2vec(sentences):\n",
    "    '''\n",
    "    word2vect의 리터하는 벡터들의 평균을 구하고, 이를 코사인 거리를 구함\n",
    "    '''\n",
    "    def get_word2vec(word2vec, words, embedding_size=200):\n",
    "        '''\n",
    "        단어 리스트를 받고, 벡터로 변환후에 모든 벡터의 평균을 리턴\n",
    "        '''\n",
    "        vectors = []\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vector = word2vec[word]    # 워드 -> 벡터\n",
    "                # print(word)\n",
    "            except:\n",
    "                print(f\"{word} : Unknowd words in word2vec\")\n",
    "                vector = np.zeros(embedding_size) # 모르는 단어이면 0으로 채움\n",
    "            # break\n",
    "            vectors.append(vector)\n",
    "            # print(vector.sum())     \n",
    "\n",
    "        avg_vectors = np.mean(vectors, axis=0)    \n",
    "        return avg_vectors\n",
    "    \n",
    "    s1 = sentences[0]\n",
    "    s2 = sentences[1]\n",
    "    ps1 = tokenizer(s1)\n",
    "    ps2 = tokenizer(s2)\n",
    "\n",
    "    avg_vector1 = get_word2vec(word2vec, ps1)\n",
    "    avg_vector2 = get_word2vec(word2vec, ps2)\n",
    "    # print(avg_vector)\n",
    "    dist_w2v = spatial.distance.cosine(avg_vector1, avg_vector2)\n",
    "    print(s1,': ', ps1)\n",
    "    print(s2,': ', ps2)    \n",
    "    print('dist_w2v: {}'.format(dist_w2v))    \n",
    "\n",
    "##################################\n",
    "# FastText Word2Vec 관련 라이브러리\n",
    "##################################\n",
    "    \n",
    "from gensim import models\n",
    "def get_fasttext_model(path):\n",
    "    '''\n",
    "    path = 'resource/cc.ko.300.bin'\n",
    "    ko_model = get_fasttext_model(path)    \n",
    "    '''    \n",
    "    try:\n",
    "        if type(ko_model) == gensim.models.fasttext.FastText:\n",
    "            print('Model is already loaded')\n",
    "        else:\n",
    "            print('Model is loading')\n",
    "            ko_model = models.fasttext.load_facebook_model(path)\n",
    "            \n",
    "    except:\n",
    "        print('Model is loading')\n",
    "        ko_model = models.fasttext.load_facebook_model(path)\n",
    "        \n",
    "    return ko_model        \n",
    "    \n",
    "def sim_sentences_fasttext_word2vec(ko_model, sentences):\n",
    "    '''\n",
    "    word2vect의 리터하는 벡터들의 평균을 구하고, 이를 코사인 거리를 구함\n",
    "    '''\n",
    "    def get_word2vec(ko_model, words, embedding_size=300):\n",
    "        '''\n",
    "        단어 리스트를 받고, 벡터로 변환후에 모든 벡터의 평균을 리턴\n",
    "        '''\n",
    "        vectors = []\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vector = ko_model.wv.get_vector(word)\n",
    "                # print(word)\n",
    "            except:\n",
    "                print(f\"{word} : Unknowd words in word2vec\")\n",
    "                vector = np.zeros(embedding_size) # 모르는 단어이면 0으로 채움\n",
    "            # break\n",
    "            vectors.append(vector)\n",
    "            # print(vector.sum())     \n",
    "\n",
    "        avg_vectors = np.mean(vectors, axis=0)    \n",
    "        return avg_vectors\n",
    "    \n",
    "    s1 = sentences[0]\n",
    "    s2 = sentences[1]\n",
    "    ps1 = tokenizer(s1)\n",
    "    ps2 = tokenizer(s2)\n",
    "\n",
    "    avg_vector1 = get_word2vec(ko_model, ps1)\n",
    "    avg_vector2 = get_word2vec(ko_model, ps2)\n",
    "    # print(avg_vector)\n",
    "    dist_w2v = spatial.distance.cosine(avg_vector1, avg_vector2)\n",
    "    print(s1,': ', ps1)\n",
    "    print(s2,': ', ps2)    \n",
    "\n",
    "    print('dist_w2v: {}'.format(dist_w2v))    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-field",
   "metadata": {},
   "source": [
    "## 테스트 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "atlantic-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 ='사과는 과일이다'\n",
    "s2 = '책은 인류가 쌓은 지식의 보고다'\n",
    "s3 = '건강에 좋은 것은 과일이다'\n",
    "# s3 ='사과는 과일이다'\n",
    "\n",
    "sents1 = [s1, s2]    \n",
    "sents2 = [s1, s3]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-shooting",
   "metadata": {},
   "source": [
    "## sent2vec 테스트\n",
    "0 에 가까울 수록 유사한 것을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "sporting-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과는 과일이다', '책은 인류가 쌓은 지식의 보고다']\n",
      "dist_1: 0.028812527656555176\n",
      "['사과는 과일이다', '건강에 좋은 것은 과일이다']\n",
      "dist_1: 0.009791731834411621\n"
     ]
    }
   ],
   "source": [
    "sim_sentences_sent2vec(sents1)\n",
    "sim_sentences_sent2vec(sents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-vinyl",
   "metadata": {},
   "source": [
    "## Kakao Word2Vec 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "peaceful-horror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쌓다 : Unknowd words in word2vec\n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['인류', '쌓다', '지식', '보고']\n",
      "dist_w2v: 0.8379042084884104\n",
      "좋다 : Unknowd words in word2vec\n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['건강', '좋다', '과일']\n",
      "dist_w2v: 0.3584424536921966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:64: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "sim_sentences_word2vec(sents1)\n",
    "sim_sentences_word2vec(sents2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-operations",
   "metadata": {},
   "source": [
    "## FastText Word2Vec 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "worst-sender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['인류', '쌓다', '지식', '보고']\n",
      "dist_w2v: 0.680292159318924\n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['건강', '좋다', '과일']\n",
      "dist_w2v: 0.3414410352706909\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ko_model == gensim.models.fasttext.FastText\n",
    "except:\n",
    "    print(\"Need to load a model\")\n",
    "    path = 'resource/cc.ko.300.bin'    \n",
    "    ko_model = get_fasttext_model(path)    \n",
    "\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents1)\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-conducting",
   "metadata": {},
   "source": [
    "## Bulk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "decimal-official",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사과는 과일이다</td>\n",
       "      <td>책은 인류가 쌓은 지식의 보고다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>사과는 과일이다</td>\n",
       "      <td>건강에 좋은 것은 과일이다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>올리반 여아용 위티컬러폴 원피스</td>\n",
       "      <td>리바이스키즈 아동용 박스탭 긴팔티셔츠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>올리반 여아용 위티컬러폴 원피스</td>\n",
       "      <td>남자 스판 부츠컷 청바지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>올리반 여아용 위티컬러폴 원피스</td>\n",
       "      <td>해맑은푸드 오징어 실채</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>남자 스판 부츠컷 청바지</td>\n",
       "      <td>해맑은푸드 오징어 실채</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sent1                 sent2\n",
       "0           사과는 과일이다     책은 인류가 쌓은 지식의 보고다\n",
       "1           사과는 과일이다        건강에 좋은 것은 과일이다\n",
       "2  올리반 여아용 위티컬러폴 원피스  리바이스키즈 아동용 박스탭 긴팔티셔츠\n",
       "3  올리반 여아용 위티컬러폴 원피스         남자 스판 부츠컷 청바지\n",
       "4  올리반 여아용 위티컬러폴 원피스          해맑은푸드 오징어 실채\n",
       "5      남자 스판 부츠컷 청바지          해맑은푸드 오징어 실채"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.getcwd()\n",
    "df = pd.read_csv('data/test_sent.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "assigned-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### 0 ######\n",
      "BERT: \n",
      "['사과는 과일이다', '책은 인류가 쌓은 지식의 보고다']\n",
      "dist_1: 0.028812527656555176\n",
      "Kakao Word2Vec: \n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "책은 인류가 쌓은 지식의 보고다 :  ['인류', '지식', '보고']\n",
      "dist_w2v: 0.8379041850566864\n",
      "FastTExt Word2Vec: \n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "책은 인류가 쌓은 지식의 보고다 :  ['인류', '지식', '보고']\n",
      "dist_w2v: 0.6964645683765411\n",
      "#### 1 ######\n",
      "BERT: \n",
      "['사과는 과일이다', '건강에 좋은 것은 과일이다']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:64: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist_1: 0.009791731834411621\n",
      "Kakao Word2Vec: \n",
      "좋다 : Unknowd words in word2vec\n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "건강에 좋은 것은 과일이다 :  ['건강', '좋다', '과일']\n",
      "dist_w2v: 0.3584424536921966\n",
      "FastTExt Word2Vec: \n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "건강에 좋은 것은 과일이다 :  ['건강', '좋다', '과일']\n",
      "dist_w2v: 0.3414410352706909\n",
      "#### 2 ######\n",
      "BERT: \n",
      "['올리반 여아용 위티컬러폴 원피스', '리바이스키즈 아동용 박스탭 긴팔티셔츠']\n",
      "dist_1: 0.05381333827972412\n",
      "Kakao Word2Vec: \n",
      "위티 : Unknowd words in word2vec\n",
      "리바이스 : Unknowd words in word2vec\n",
      "올리반 여아용 위티컬러폴 원피스 :  ['위티', '컬러', '원피스']\n",
      "리바이스키즈 아동용 박스탭 긴팔티셔츠 :  ['리바이스', '키즈', '아동', '박스', '티셔츠']\n",
      "dist_w2v: 0.4554945856780387\n",
      "FastTExt Word2Vec: \n",
      "올리반 여아용 위티컬러폴 원피스 :  ['위티', '컬러', '원피스']\n",
      "리바이스키즈 아동용 박스탭 긴팔티셔츠 :  ['리바이스', '키즈', '아동', '박스', '티셔츠']\n",
      "dist_w2v: 0.5177888572216034\n",
      "#### 3 ######\n",
      "BERT: \n",
      "['올리반 여아용 위티컬러폴 원피스', '남자 스판 부츠컷 청바지']\n",
      "dist_1: 0.01535344123840332\n",
      "Kakao Word2Vec: \n",
      "위티 : Unknowd words in word2vec\n",
      "올리반 여아용 위티컬러폴 원피스 :  ['위티', '컬러', '원피스']\n",
      "남자 스판 부츠컷 청바지 :  ['남자', '스판', '부츠', '청바지']\n",
      "dist_w2v: 0.6317759716398765\n",
      "FastTExt Word2Vec: \n",
      "올리반 여아용 위티컬러폴 원피스 :  ['위티', '컬러', '원피스']\n",
      "남자 스판 부츠컷 청바지 :  ['남자', '스판', '부츠', '청바지']\n",
      "dist_w2v: 0.5383302867412567\n",
      "#### 4 ######\n",
      "BERT: \n",
      "['올리반 여아용 위티컬러폴 원피스', '해맑은푸드 오징어 실채']\n",
      "dist_1: 0.030350863933563232\n",
      "Kakao Word2Vec: \n",
      "위티 : Unknowd words in word2vec\n",
      "실채 : Unknowd words in word2vec\n",
      "올리반 여아용 위티컬러폴 원피스 :  ['위티', '컬러', '원피스']\n",
      "해맑은푸드 오징어 실채 :  ['맑은', '푸드', '오징어', '실채']\n",
      "dist_w2v: 0.6672230296176676\n",
      "FastTExt Word2Vec: \n",
      "올리반 여아용 위티컬러폴 원피스 :  ['위티', '컬러', '원피스']\n",
      "해맑은푸드 오징어 실채 :  ['맑은', '푸드', '오징어', '실채']\n",
      "dist_w2v: 0.5954355597496033\n",
      "#### 5 ######\n",
      "BERT: \n",
      "['남자 스판 부츠컷 청바지', '해맑은푸드 오징어 실채']\n",
      "dist_1: 0.015106141567230225\n",
      "Kakao Word2Vec: \n",
      "실채 : Unknowd words in word2vec\n",
      "남자 스판 부츠컷 청바지 :  ['남자', '스판', '부츠', '청바지']\n",
      "해맑은푸드 오징어 실채 :  ['맑은', '푸드', '오징어', '실채']\n",
      "dist_w2v: 0.6775496089051047\n",
      "FastTExt Word2Vec: \n",
      "남자 스판 부츠컷 청바지 :  ['남자', '스판', '부츠', '청바지']\n",
      "해맑은푸드 오징어 실채 :  ['맑은', '푸드', '오징어', '실채']\n",
      "dist_w2v: 0.6519558131694794\n"
     ]
    }
   ],
   "source": [
    "for index, sent in df.iterrows():\n",
    "    print(f'#### {index} ######' )\n",
    "    sents1 = [sent[0], sent[1]]    \n",
    "    print(\"BERT: \");sim_sentences_sent2vec(sents1)   ## BERT\n",
    "    print(\"Kakao Word2Vec: \"); sim_sentences_word2vec(sents1)   ## Kakao Word2Vec \n",
    "    print(\"FastTExt Word2Vec: \");sim_sentences_fasttext_word2vec(ko_model, sents1)    ## FastText Word2Vec\n",
    "    if (index == 5):\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-commons",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
