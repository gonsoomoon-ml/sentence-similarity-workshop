{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "possible-browser",
   "metadata": {},
   "source": [
    "# send2vec, word2vect 테스트\n",
    "- How to Compute Sentence Similarity Using BERT and Word2Vec\n",
    "    - https://towardsdatascience.com/how-to-compute-sentence-similarity-using-bert-and-word2vec-ab0663a5d64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "surrounded-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# sent2vec 라이브러리\n",
    "##################################\n",
    "\n",
    "from sent2vec.vectorizer import Vectorizer\n",
    "from scipy import spatial\n",
    "\n",
    "def sim_sentences_sent2vec(sentences):\n",
    "    vectorizer = Vectorizer()\n",
    "    print(sentences)\n",
    "    vectorizer.bert(sentences)\n",
    "    vectors = vectorizer.vectors    \n",
    "    dist_1 = spatial.distance.cosine(vectors[0], vectors[1])\n",
    "    # dist_2 = spatial.distance.cosine(vectors[0], vectors[2])\n",
    "    # print('dist_1: {0}, dist_2: {1}'.format(dist_1, dist_2))\n",
    "    print('dist_1: {0}'.format(dist_1))    \n",
    "    # dist_1: 0.043, dist_2: 0.192\n",
    "\n",
    "    \n",
    "##################################\n",
    "# KoNLpy 토큰라이저\n",
    "##################################\n",
    "from konlpy.tag import Okt\n",
    "Okt = Okt()\n",
    "pretrained_kr_word2vec = 'resource/ko.bin'\n",
    "# datapath = 'ko.bin'\n",
    "datapath = pretrained_kr_word2vec\n",
    "# print(datapath)\n",
    " \n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "word2vec = gensim.models.Word2Vec.load(datapath)\n",
    "\n",
    "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,형용사, 부사, 알파벳 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Adjective\", \"Adverb\"], stopword=[]):\n",
    "    return [\n",
    "        word for word, tag in Okt.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]\n",
    "\n",
    "    \n",
    "##################################\n",
    "# Kakao 개발자 Word2Vec 관련 라이브러리\n",
    "##################################\n",
    "\n",
    "    \n",
    "def sim_sentences_word2vec(sentences):\n",
    "    '''\n",
    "    word2vect의 리터하는 벡터들의 평균을 구하고, 이를 코사인 거리를 구함\n",
    "    '''\n",
    "    def get_word2vec(word2vec, words, embedding_size=200):\n",
    "        '''\n",
    "        단어 리스트를 받고, 벡터로 변환후에 모든 벡터의 평균을 리턴\n",
    "        '''\n",
    "        vectors = []\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vector = word2vec[word]    # 워드 -> 벡터\n",
    "                # print(word)\n",
    "            except:\n",
    "                # print(\"Unknowd words\")\n",
    "                vector = np.zeros(embedding_size) # 모르는 단어이면 0으로 채움\n",
    "            # break\n",
    "            vectors.append(vector)\n",
    "            # print(vector.sum())     \n",
    "\n",
    "        avg_vectors = np.mean(vectors, axis=0)    \n",
    "        return avg_vectors\n",
    "    \n",
    "    s1 = sentences[0]\n",
    "    s2 = sentences[1]\n",
    "    ps1 = tokenizer(s1)\n",
    "    ps2 = tokenizer(s2)\n",
    "\n",
    "    avg_vector1 = get_word2vec(word2vec, ps1)\n",
    "    avg_vector2 = get_word2vec(word2vec, ps2)\n",
    "    # print(avg_vector)\n",
    "    dist_w2v = spatial.distance.cosine(avg_vector1, avg_vector2)\n",
    "    print(s1,': ', ps1)\n",
    "    print(s1,': ', ps2)    \n",
    "    print('dist_w2v: {}'.format(dist_w2v))    \n",
    "\n",
    "##################################\n",
    "# FastText Word2Vec 관련 라이브러리\n",
    "##################################\n",
    "    \n",
    "from gensim import models\n",
    "def get_fasttext_model(path):\n",
    "    '''\n",
    "    path = 'resource/cc.ko.300.bin'\n",
    "    ko_model = get_fasttext_model(path)    \n",
    "    '''    \n",
    "    try:\n",
    "        if type(ko_model) == gensim.models.fasttext.FastText:\n",
    "            print('Model is already loaded')\n",
    "        else:\n",
    "            print('Model is loading')\n",
    "            ko_model = models.fasttext.load_facebook_model(path)\n",
    "            \n",
    "    except:\n",
    "        print('Model is loading')\n",
    "        ko_model = models.fasttext.load_facebook_model(path)\n",
    "        \n",
    "    return ko_model        \n",
    "    \n",
    "def sim_sentences_fasttext_word2vec(ko_model, sentences):\n",
    "    '''\n",
    "    word2vect의 리터하는 벡터들의 평균을 구하고, 이를 코사인 거리를 구함\n",
    "    '''\n",
    "    def get_word2vec(ko_model, words, embedding_size=300):\n",
    "        '''\n",
    "        단어 리스트를 받고, 벡터로 변환후에 모든 벡터의 평균을 리턴\n",
    "        '''\n",
    "        vectors = []\n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                vector = ko_model.wv.get_vector(word)\n",
    "                # print(word)\n",
    "            except:\n",
    "                # print(\"Unknowd words\")\n",
    "                vector = np.zeros(embedding_size) # 모르는 단어이면 0으로 채움\n",
    "            # break\n",
    "            vectors.append(vector)\n",
    "            # print(vector.sum())     \n",
    "\n",
    "        avg_vectors = np.mean(vectors, axis=0)    \n",
    "        return avg_vectors\n",
    "    \n",
    "    s1 = sentences[0]\n",
    "    s2 = sentences[1]\n",
    "    ps1 = tokenizer(s1)\n",
    "    ps2 = tokenizer(s2)\n",
    "\n",
    "    avg_vector1 = get_word2vec(ko_model, ps1)\n",
    "    avg_vector2 = get_word2vec(ko_model, ps2)\n",
    "    # print(avg_vector)\n",
    "    dist_w2v = spatial.distance.cosine(avg_vector1, avg_vector2)\n",
    "    print(s1,': ', ps1)\n",
    "    print(s1,': ', ps2)    \n",
    "\n",
    "    print('dist_w2v: {}'.format(dist_w2v))    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-covering",
   "metadata": {},
   "source": [
    "## 테스트 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "quarterly-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 ='사과는 과일이다'\n",
    "s2 = '책은 인류가 쌓은 지식의 보고다'\n",
    "s3 = '건강에 좋은 것은 과일이다'\n",
    "# s3 ='사과는 과일이다'\n",
    "\n",
    "sents1 = [s1, s2]    \n",
    "sents2 = [s1, s3]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-standard",
   "metadata": {},
   "source": [
    "## sent2vec 테스트\n",
    "0 에 가까울 수록 유사한 것을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "corresponding-administrator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사과는 과일이다', '책은 인류가 쌓은 지식의 보고다']\n",
      "dist_1: 0.028812527656555176\n",
      "['사과는 과일이다', '건강에 좋은 것은 과일이다']\n",
      "dist_1: 0.009791731834411621\n"
     ]
    }
   ],
   "source": [
    "sim_sentences_sent2vec(sents1)\n",
    "sim_sentences_sent2vec(sents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-gardening",
   "metadata": {},
   "source": [
    "## Kakao Word2Vec 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "buried-chester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['인류', '쌓다', '지식', '보고']\n",
      "dist_w2v: 0.8379042084884104\n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['건강', '좋다', '과일']\n",
      "dist_w2v: 0.3584424536921966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:64: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "sim_sentences_word2vec(sents1)\n",
    "sim_sentences_word2vec(sents2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-machinery",
   "metadata": {},
   "source": [
    "## FastText Word2Vec 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "domestic-explosion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['인류', '쌓다', '지식', '보고']\n",
      "dist_w2v: 0.680292159318924\n",
      "사과는 과일이다 :  ['사과', '과일']\n",
      "사과는 과일이다 :  ['건강', '좋다', '과일']\n",
      "dist_w2v: 0.3414410352706909\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ko_model == gensim.models.fasttext.FastText\n",
    "except:\n",
    "    print(\"Need to load a model\")\n",
    "    path = 'resource/cc.ko.300.bin'    \n",
    "    ko_model = get_fasttext_model(path)    \n",
    "\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents1)\n",
    "sim_sentences_fasttext_word2vec(ko_model, sents2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-memphis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
